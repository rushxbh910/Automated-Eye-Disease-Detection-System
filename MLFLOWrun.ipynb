{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the tracking server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand the MLFlow tracking server system\n",
    "The “remote tracking server” system includes:\n",
    "\n",
    "-   a database Postgre sql =------ in which to store structured data for each “run”, like the start and end time, hyperparameter values, and the values of metrics that we log to the server. In our deploymenet, this will be realized by a PostgreSQL server.\n",
    "-   an object store  =------ in which MLFlow will log artifacts - model weights, images (e.g. PNGs), and so on. In our deployment, this will be realized by MinIO, an open source object storage system that is compatible with AWS S3 APIs (so it may be used as a drop-in self-managed replacement for AWS S3).\n",
    "-   the MLFlow tracking server \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start MLFlow tracking server system\n",
    "\n",
    "Now we are ready to get it started! Bring up our MLFlow system with:\n",
    "\n",
    "``` bash\n",
    "# run on node-mltrain\n",
    "docker compose -f Data_eye/Docker/docker-compose-mlflow.yaml up -d\n",
    "```\n",
    "\n",
    "\n",
    "When it is finished, the output of\n",
    "\n",
    "``` bash\n",
    "# run on node-mltrain\n",
    "docker ps\n",
    "```\n",
    "\n",
    "should show that the `minio`, `postgres`, and `mlflow` containers are running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access dashboards for the MLFlow tracking server system\n",
    "\n",
    "The MinIO dashboard \n",
    "\n",
    "    http://129.114.108.94:9001\n",
    "\n",
    "MINIO_ROOT_USER: \"Project24-MINIO-Id\"\n",
    "      MINIO_ROOT_PASSWORD: \"Project-24-MINIO-Id-secret-key\"\n",
    "\n",
    "Log in with the credentials we specified in the Docker Compose YAML:\n",
    "\n",
    "-   Username: `your-access-key`\n",
    "-   Password: `your-secret-key`\n",
    "\n",
    "\n",
    "Next, let’s look at the MLFlow UI. This runs on port 8000. In a browser, open\n",
    "\n",
    "    http://129.114.108.94:8000\n",
    "AWS_ACCESS_KEY_ID: \"Project24-MLFLOW-Id\"\n",
    "      AWS_SECRET_ACCESS_KEY: \"Project-24-MINIO-Id-secret-key\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Start a Jupyter server\n",
    "\n",
    "Finally, we’ll start the Jupyter server container, inside which we will run experiments that are tracked in MLFlow. Make sure your container image build, from the previous section, is now finished - you should see a “jupyter-mlflow” image in the output of:\n",
    "\n",
    "``` bash\n",
    "“jupyter-mlflow” image in the output of:\n",
    "# run on node-mltrain\n",
    "docker image list\n",
    "```\n",
    "\n",
    "The command to run will depend on what type of GPU node you are using -\n",
    "\n",
    "If you are using an AMD GPU (node type `gpu_mi100`), run\n",
    "\n",
    "``` bash\n",
    "# run on node-mltrain IF it is a gpu_mi100\n",
    "HOST_IP=$(curl --silent http://169.254.169.254/latest/meta-data/public-ipv4 )\n",
    "docker run  -d --rm  -p 8888:8888 \\\n",
    "    --device=/dev/kfd --device=/dev/dri \\\n",
    "    --group-add video --group-add $(getent group | grep render | cut -d':' -f 3) \\\n",
    "    --shm-size 16G \\\n",
    "    -v ~/mltrain-chi/workspace_mlflow:/home/jovyan/work/ \\\n",
    "    -v food11:/mnt/ \\\n",
    "    -e MLFLOW_TRACKING_URI=http://${HOST_IP}:8000/ \\\n",
    "    -e FOOD11_DATA_DIR=/mnt/Food-11 \\\n",
    "    --name jupyter \\\n",
    "    jupyter-mlflow\n",
    "```\n",
    "``` bash\n",
    "# run on node-mltrain IF it is a gpu_mi100\n",
    "HOST_IP=$(curl --silent http://169.254.169.254/latest/meta-data/public-ipv4 )\n",
    "docker run  -d --rm  -p 8888:8888 \\\n",
    "    --device=/dev/kfd --device=/dev/dri \\\n",
    "    --group-add video --group-add $(getent group | grep render | cut -d':' -f 3) \\\n",
    "    --shm-size 16G \\\n",
    "    -v ~/workspace/workspace_mlflow:/home/jovyan/work/ \\\n",
    "    -v EYE:/mnt/ \\\n",
    "    -e MLFLOW_TRACKING_URI=http://${HOST_IP}:8000/ \\\n",
    "    -e EYE_DATA_DIR=/mnt/eye_dataset \\\n",
    "    --name jupyter \\\n",
    "    jupyter-mlflow\n",
    "```\n",
    "Note that we intially get `HOST_IP`, the floating IP assigned to your instance, as a variable; then we use it to specify the `MLFLOW_TRACKING_URI` inside the container. Training jobs inside the container will access the MLFlow tracking server using its public IP address.\n",
    "\n",
    "Here,\n",
    "\n",
    "-   `-d` says to start the container and detach, leaving it running in the background\n",
    "-   `-rm` says that after we stop the container, it should be removed immediately, instead of leaving it around for potential debugging\n",
    "-   `-p 8888:8888` says to publish the container’s port `8888` (the second `8888` in the argument) to the host port `8888` (the first `8888` in the argument)\n",
    "-   `--device=/dev/kfd --device=/dev/dri` pass the AMD GPUs to the container\n",
    "-   `--group-add video --group-add $(getent group | grep render | cut -d':' -f 3)` makes sure that the user inside the container is a member of a group that has permission to use the GPU(s) - the `video` group and the `render` group. (The `video` group always has the same group ID, by convention, but [the `render` group does not](https://github.com/ROCm/ROCm-docker/issues/90), so we need to find out its group ID on the host and pass that to the container.)\n",
    "-   `--shm-size 16G` increases the memory available for interprocess communication\n",
    "-   the host directory `~/mltrain-chi/workspace_mlflow` is mounted inside the workspace as `/home/jovyan/work/`\n",
    "-   the volume `food11` is mounted inside the workspace as `/mnt/`\n",
    "-   and we pass `MLFLOW_TRACKING_URI` and `FOOD11_DATA_DIR` as environment variables.\n",
    "\n",
    "If you are using an NVIDIA GPU (node type `compute_liqid`), run\n",
    "\n",
    "``` bash\n",
    "# run on node-mltrain IF it is a compute_liqid\n",
    "HOST_IP=$(curl --silent http://169.254.169.254/latest/meta-data/public-ipv4 )\n",
    "docker run  -d --rm  -p 8888:8888 \\\n",
    "    --gpus all \\\n",
    "    --shm-size 16G \\\n",
    "    -v ~/workspace/workspace_mlflow:/home/jovyan/work/ \\\n",
    "    -v EYE:/mnt/ \\\n",
    "    -e MLFLOW_TRACKING_URI=http://${HOST_IP}:8000/ \\\n",
    "    -e EYE_DATA_DIR=/mnt/eye_dataset \\\n",
    "    --name jupyter \\\n",
    "    jupyter-mlflow\n",
    "```\n",
    "HOST_IP=$(curl --silent http://169.254.169.254/latest/meta-data/public-ipv4 )\n",
    "docker run  -d --rm  -p 8888:8888 \\\n",
    "    --gpus all \\\n",
    "    --shm-size 16G \\\n",
    "    -v ~/mltrain-chi/workspace_mlflow:/home/jovyan/work/ \\\n",
    "    -v food11:/mnt/ \\\n",
    "    -e MLFLOW_TRACKING_URI=http://${HOST_IP}:8000/ \\\n",
    "    -e FOOD11_DATA_DIR=/mnt/Food-11 \\\n",
    "    --name jupyter \\\n",
    "    jupyter-mlflow\n",
    "    \n",
    "\n",
    "Note that we intially get `HOST_IP`, the floating IP assigned to your instance, as a variable; then we use it to specify the `MLFLOW_TRACKING_URI` inside the container. Training jobs inside the container will access the MLFlow tracking server using its public IP address.\n",
    "\n",
    "-   `-d` says to start the container and detach, leaving it running in the background\n",
    "-   `-rm` says that after we stop the container, it should be removed immediately, instead of leaving it around for potential debugging\n",
    "-   `-p 8888:8888` says to publish the container’s port `8888` (the second `8888` in the argument) to the host port `8888` (the first `8888` in the argument)\n",
    "-   `--gus all` pass the NVIDIA GPUs to the container\n",
    "-   `--shm-size 16G` increases the memory available for interprocess communication\n",
    "-   the host directory `~/mltrain-chi/workspace_mlflow` is mounted inside the workspace as `/home/jovyan/work/`\n",
    "-   the volume `food11` is mounted inside the workspace as `/mnt/`\n",
    "-   and we pass `MLFLOW_TRACKING_URI` and `FOOD11_DATA_DIR` as environment variables.\n",
    "\n",
    "Then, run\n",
    "\n",
    "    docker logs jupyter\n",
    "\n",
    "and look for a line like\n",
    "\n",
    "    http://127.0.0.1:8888/lab?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "\n",
    "Paste this into a browser tab, but in place of `127.0.0.1`, substitute the floating IP assigned to your instance, to open the Jupyter notebook interface.\n",
    "\n",
    "In the file browser on the left side, open the `work` directory.\n",
    "\n",
    "Open a terminal (“File \\> New \\> Terminal”) inside the Jupyter server environment, and in this terminal, run\n",
    "\n",
    "``` bash\n",
    "# runs on jupyter container inside node-mltrain\n",
    "env\n",
    "```\n",
    "\n",
    "to see environment variables. Confirm that the `MLFLOW_TRACKING_URI` is set, with the correct floating IP address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
