# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uiVzCgie_6WX0LkKcxeKIRqsBq5EmNlM
"""

from chi import server, context, lease
import chi, os, time, datetime

context.version = "1.0"
context.choose_project()
context.choose_site(default="CHI@TACC")

"""## 2. Obtain and Verify Lease

Check your existing lease details to ensure you have an active allocation.
"""

l = lease.get_lease("Project2400")
l.show()

"""## 3. Create and Launch Server

Launch a server with Ubuntu 24.04 and CUDA support.
"""

username = os.getenv('USER') # all exp resources will have this prefix
s = server.Server(
    f"node-sb9880",
    reservation_id=l.node_reservations[0]["id"],
    image_name="CC-Ubuntu24.04-CUDA"
)
s.submit(idempotent=True)

"""## 4. Configure Network Access

Set up network access to make our server accessible.
"""

# Associate a floating IP to make the server accessible from the internet
s.associate_floating_ip()

# Refresh server info and verify connectivity
s.refresh()
s.check_connectivity()

"""### Configure Security Groups

By default, all connections to VM resources are blocked as a security measure. We need to attach security groups to our VM to permit access over the Internet to specified ports.
"""

# Define security groups for required services
security_groups = [
  {'name': "allow-ssh", 'port': 22, 'description': "Enable SSH traffic on TCP port 22"},
  {'name': "allow-8888", 'port': 8888, 'description': "Enable TCP port 8888 (used by Jupyter)"},
  {'name': "allow-8000", 'port': 8000, 'description': "Enable TCP port 8000 (used by MLFlow)"},
  {'name': "allow-9000", 'port': 9000, 'description': "Enable TCP port 9000 (used by MinIO API)"},
  {'name': "allow-9001", 'port': 9001, 'description': "Enable TCP port 9001 (used by MinIO Web UI)"}
]

# Configure openstacksdk for actions unsupported by python-chi
os_conn = chi.clients.connection()
nova_server = chi.nova().servers.get(s.id)

for sg in security_groups:
  # Create security group if it doesn't exist
  if not os_conn.get_security_group(sg['name']):
      os_conn.create_security_group(sg['name'], sg['description'])
      os_conn.create_security_group_rule(sg['name'], port_range_min=sg['port'], port_range_max=sg['port'],
                                         protocol='tcp', remote_ip_prefix='0.0.0.0/0')
  # Add security group to server
  nova_server.add_security_group(sg['name'])

print(f"Updated security groups: {[group.name for group in nova_server.list_security_group()]}")

"""## 5. Install Docker

Set up Docker to run our containerized environments.
"""

# Install Docker using the official installation script
s.execute("curl -sSL https://get.docker.com/ | sudo sh")

# Add current user to docker group to run docker without sudo
s.execute("sudo groupadd -f docker; sudo usermod -aG docker $USER")

"""## 6. GPU Setup

Set up GPU support for Docker based on your hardware. Choose the appropriate section (NVIDIA or AMD) based on your hardware.

### 6A. NVIDIA GPU Setup

Configure Docker to work with NVIDIA GPUs.
"""

# Install NVIDIA Container Toolkit
s.execute("curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \
  && curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
    sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
    sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list")
s.execute("sudo apt update")
s.execute("sudo apt-get install -y nvidia-container-toolkit")
s.execute("sudo nvidia-ctk runtime configure --runtime=docker")

# Fix for https://github.com/NVIDIA/nvidia-container-toolkit/issues/48
s.execute("sudo jq 'if has(\"exec-opts\") then . else . + {\"exec-opts\": [\"native.cgroupdriver=cgroupfs\"]} end' /etc/docker/daemon.json | sudo tee /etc/docker/daemon.json.tmp > /dev/null && sudo mv /etc/docker/daemon.json.tmp /etc/docker/daemon.json")

# Restart Docker service to apply changes
s.execute("sudo systemctl restart docker")

# Install nvtop for monitoring NVIDIA GPUs
s.execute("sudo apt update")
s.execute("sudo apt -y install nvtop")

"""### 6B. AMD GPU Setup

Configure system to work with AMD GPUs.
"""

# Install AMD GPU driver using amdgpu-install
s.execute("sudo apt update; wget https://repo.radeon.com/amdgpu-install/6.3.3/ubuntu/noble/amdgpu-install_6.3.60303-1_all.deb")
s.execute("sudo apt -y install ./amdgpu-install_6.3.60303-1_all.deb; sudo apt update")
s.execute("amdgpu-install -y --usecase=dkms")

# Install rocm-smi for GPU monitoring
s.execute("sudo apt -y install rocm-smi")

# Add user to video and render groups for GPU access
s.execute("sudo usermod -aG video,render $USER")

# Reboot to apply changes
s.execute("sudo reboot")
time.sleep(30)  # Wait for server to come back online

# Verify connectivity after reboot
s.refresh()
s.check_connectivity()

# Verify AMD GPU is working
s.execute("rocm-smi")

# Install nvtop with AMD GPU support
s.execute("sudo apt -y install cmake libncurses-dev libsystemd-dev libudev-dev libdrm-dev libgtest-dev")
s.execute("git clone https://github.com/Syllo/nvtop")
s.execute("mkdir -p nvtop/build && cd nvtop/build && cmake .. -DAMDGPU_SUPPORT=ON && sudo make install")

"""## 7. Build MLFlow Container Image

Create a Docker image that includes Jupyter, PyTorch, and MLFlow with CUDA support.
"""

s.execute("git clone --recurse-submodules https://github.com/sb9880/Data_eye")

# Clone repository with Dockerfile and configuration
#s.execute("git clone --recurse-submodules https://github.com/sb9880/Data_eye")

# Build the Docker image
s.execute("docker build -t jupyter-mlflow -f Automated-Eye-Disease-Detection-System/docker/Dockerfile.jupyter-torch-mlflow-cuda .")
s.execute("docker build -t ray-rocm:2.42.1 -f Automated-Eye-Disease-Detection-System/docker/Dockerfile.ray-rocm .")
s.execute("docker build -t jupyter-ray -f Automated-Eye-Disease-Detection-System/docker/Dockerfile.jupyter-ray .")

print("done")

"""## 8. Access Your Environment

Now you can SSH into your server using the floating IP address that was assigned.

```bash
# Replace with your assigned IP address
ssh -i ~/.ssh/id_rsa_chameleon cc@129.114.108.94
```

## 9. Set Up Persistent Storage

Configure object storage for persisting ML models and data.

### 9.1 Install and Configure Rclone

Rclone allows you to mount object storage as a local filesystem.

```bash
# Run on your server
curl https://rclone.org/install.sh | sudo bash

# Enable user_allow_other in FUSE configuration
sudo sed -i '/^#user_allow_other/s/^#//' /etc/fuse.conf

# Create rclone config directory
mkdir -p ~/.config/rclone
```
"""

# Run on your server
s.execute("curl https://rclone.org/install.sh | sudo bash")

# Enable user_allow_other in FUSE configuration
s.execute("sudo sed -i '/^#user_allow_other/s/^#//' /etc/fuse.conf")

# Create rclone config directory
s.execute("mkdir -p ~/.config/rclone")

"""### 9.2 Create Rclone Configuration

Create a configuration file for accessing Chameleon object storage:

```bash
# Edit rclone config file
mkdir -p ~/.config/rclone
nano ~/.config/rclone/rclone.conf
```

Add the following to the config file (replace with your credentials):

```
[chi_tacc]
type = swift
user_id = YOUR_USER_ID
application_credential_id = APP_CRED_ID
application_credential_secret = APP_CRED_SECRET
auth = https://chi.tacc.chameleoncloud.org:5000/v3
region = CHI@TACC
```

Test the configuration:

```bash
rclone lsd chi_tacc:
```
"""

s.execute("mkdir -p ~/.config/rclone")

s.execute('''
cat > ~/.config/rclone/rclone.conf << EOF
[chi_tacc]
type = swift
user_id = e2577330508e3973cfe043571304aac84303c56c5f6e3388146f5a29a70065c7
application_credential_id = ec481a935673452a9fc7a565564830d9
application_credential_secret = TaLTF-J24JePMabTxzrAw4H0RaTM6rDDEw9PSh3PF2Ao4VxynLFPp8z7PtV8GyFX5XehqnCsBQEyHE7RJKgvyQ
auth = https://chi.tacc.chameleoncloud.org:5000/v3
region = CHI@TACC
EOF''')
s.execute("rclone lsd chi_tacc:")

"""### 9.3 Mount Object Storage

Mount your object container as a local filesystem:

```bash
# Create mount point
sudo mkdir -p /mnt/object
sudo chown -R cc /mnt/object
sudo chgrp -R cc /mnt/object

# Mount object storage (replace with your container name)
rclone mount chi_tacc:object-persist-project24 /mnt/object --read-only --allow-other --daemon

# Verify mount
ls /mnt/object
```
"""

# Create mount point
s.execute("sudo mkdir -p /mnt/object")
s.execute("sudo chown -R cc /mnt/object")
s.execute("sudo chgrp -R cc /mnt/object")

# Mount object storage (replace with your container name)
s.execute("rclone mount chi_tacc:object-persist-project24 /mnt/object --read-only --allow-other --daemon")

# Verify mount
s.execute("ls /mnt/object")

"""## 10. Dataset Setup with Docker

Create and populate a Docker volume with the Food-11 dataset.

```bash
# Create Docker volume for data
docker volume create EYE

# Populate volume with Food-11 dataset
#docker compose -f /Data_eye/Docker/docker-compose-data.yaml up -d

# Verify data is correctly loaded
docker run --rm -it -v EYE:/mnt alpine ls -l /mnt/transformed_eye_dataset/
```
"""

s.execute("docker volume create EYE")
#s.execute("docker compose -f /Automated-Eye-Disease-Detection-System/docker/docker-compose-data.yaml up -d")

s.execute("""export HOST_IP=$(curl --silent http://169.254.169.254/latest/meta-data/public-ipv4) &&
          docker compose -f Automated-Eye-Disease-Detection-System/docker/docker-compose-ray-mlflow-data.yaml up -d""")

s.execute("docker run --rm -v EYE:/mnt alpine ls -l /mnt/")

s.execute('''
HOST_IP=$(curl --silent http://169.254.169.254/latest/meta-data/public-ipv4 )
docker run -d --rm -p 8888:8888 \\
    -v ~/workspace:/home/jovyan/work/ \\
    -v EYE:/mnt/ \\
    -e RAY_ADDRESS=http://${HOST_IP}:8265/ \\
    -e MLFLOW_TRACKING_URI=http://${HOST_IP}:8000/ \\
    -e EYE_DATA_DIR=/mnt/transformed_eye_dataset \\
    --name jupyter \\
    jupyter-ray
''')

#s.execute("docker logs jupyter")

s.execute('docker exec jupyter bash -c "ray job submit --runtime-env runtime.json --entrypoint-num-gpus 1 --entrypoint-num-cpus 8 --verbose --working-dir code/ -- python AugRayMLflow.py"')

s.execute("docker cp jupyter:/home/jovyan/densenet121_model.pth . && rclone copy densenet121_model.pth chi_tacc:object-persist-project24/")

s.execute("fusermount -u /mnt/object")

"""# Run Juppyter notebook
```bash
HOST_IP=$(curl --silent http://169.254.169.254/latest/meta-data/public-ipv4 )
docker run  -d --rm  -p 8888:8888 \
    --device=/dev/kfd --device=/dev/dri \
    --group-add video --group-add $(getent group | grep render | cut -d':' -f 3) \
    --shm-size 16G \
    -v ~/workspace/:/home/jovyan/work/ \
    -v EYE:/mnt/ \
    -e MLFLOW_TRACKING_URI=http://${HOST_IP}:8000/ \
    -e EYE_DATA_DIR=/mnt/transformed_eye_dataset \
    --name jupyter \
    jupyter-mlflow
    
    
    
    
    
HOST_IP=$(curl --silent http://169.254.169.254/latest/meta-data/public-ipv4 )
docker run -d --rm -p 8888:8888 \
    -v ~/workspace:/home/jovyan/work/ \
    -v EYE:/mnt/ \
    -e RAY_ADDRESS=http://${HOST_IP}:8265/ \
    -e MLFLOW_TRACKING_URI=http://${HOST_IP}:8000/ \
    -e EYE_DATA_DIR=/mnt/transformed_eye_dataset \
    --name jupyter \
    jupyter-ray
```
"""

Jupyter script ray n mlflow

docker build -t jupyter-ray -f mltrain-chi/docker/Dockerfile.jupyter-ray .




ray job submit --runtime-env runtime.json --entrypoint-num-gpus 1 --entrypoint-num-cpus 8 --verbose  --working-dir .  -- python Ray-code00

"""## 11. Cleanup - Unmount Object Storage (When Needed)

When you're done working with the object storage, you can unmount it:

```bash
# Only run this when you're done with the object storage
fusermount -u /mnt/object
```

## 12. Conclusion

You now have a fully configured MLFlow environment with:
- GPU support (NVIDIA or AMD)
- Docker containerization
- Persistent storage via object storage
- Dataset access through Docker volumes

You can access the Jupyter notebook and MLFlow interfaces through your browser using the floating IP address of your server:
- Jupyter: http://your-floating-ip:8888
- MLFlow: http://your-floating-ip:8000
- MinIO Web UI: http://your-floating-ip:9001
"""
